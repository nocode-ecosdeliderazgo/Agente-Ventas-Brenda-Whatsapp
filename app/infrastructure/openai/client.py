"""
Cliente OpenAI para integraciÃ³n con GPT-4o-mini.
Maneja anÃ¡lisis de intenciÃ³n, extracciÃ³n de informaciÃ³n y generaciÃ³n de respuestas.
"""
import logging
import json
from typing import Dict, Any, Optional
from openai import AsyncOpenAI

from app.config import settings
from prompts.agent_prompts import (
    get_intent_analysis_prompt,
    get_information_extraction_prompt,
    get_response_generation_prompt,
    get_validation_prompt,
    PromptConfig
)

logger = logging.getLogger(__name__)

def debug_print(message: str, function_name: str = "", file_name: str = "openai_client.py"):
    """Print de debug visual para consola"""
    print(f"ğŸ¤– [{file_name}::{function_name}] {message}")


def clean_openai_json_response(content: str) -> str:
    """
    Limpia respuesta de OpenAI removiendo markdown wrapping.
    
    OpenAI puede devolver JSON envuelto en ```json``` que falla el parsing.
    Esta funciÃ³n remueve ese wrapping para obtener JSON puro.
    
    Args:
        content: Contenido recibido de OpenAI
        
    Returns:
        JSON limpio sin markdown wrapping
    """
    if not content:
        return content
        
    content = content.strip()
    
    # Remover wrapping de markdown JSON
    if content.startswith('```json'):
        content = content.replace('```json\n', '').replace('```json', '')
    
    if content.endswith('```'):
        content = content.replace('\n```', '').replace('```', '')
    
    # Remover wrapping de markdown genÃ©rico
    if content.startswith('```'):
        lines = content.split('\n')
        if len(lines) > 1:
            content = '\n'.join(lines[1:])  # Remover primera lÃ­nea
    
    if content.endswith('```'):
        lines = content.split('\n')
        if len(lines) > 1:
            content = '\n'.join(lines[:-1])  # Remover Ãºltima lÃ­nea
    
    return content.strip()


class OpenAIClient:
    """
    Cliente especializado para interacciones con OpenAI GPT-4o-mini.
    
    Responsabilidades:
    - AnÃ¡lisis de intenciÃ³n de mensajes
    - ExtracciÃ³n de informaciÃ³n de usuarios
    - GeneraciÃ³n de respuestas inteligentes
    - Manejo de errores y configuraciÃ³n
    """
    
    def __init__(self):
        """Inicializa el cliente OpenAI con configuraciÃ³n."""
        if not settings.openai_api_key:
            raise ValueError("OPENAI_API_KEY no estÃ¡ configurada")
        
        self.client = AsyncOpenAI(
            api_key=settings.openai_api_key
        )
        self.logger = logging.getLogger(__name__)
    
    async def analyze_intent(
        self,
        user_message: str,
        user_memory,
        recent_messages: Optional[list] = None
    ) -> Dict[str, Any]:
        """
        Analiza la intenciÃ³n del mensaje del usuario.
        
        Args:
            user_message: Mensaje del usuario a analizar
            user_memory: Memoria del usuario con contexto
            recent_messages: Mensajes recientes para contexto
            
        Returns:
            Dict con anÃ¡lisis de intenciÃ³n estructurado
        """
        try:
            debug_print(f"ğŸ” ANALIZANDO INTENCIÃ“N\nğŸ’¬ Mensaje: '{user_message}'\nğŸ‘¤ Usuario: {user_memory.name if user_memory.name else 'AnÃ³nimo'}", "analyze_intent", "openai_client.py")
            
            prompt = get_intent_analysis_prompt(user_message, user_memory, recent_messages)
            config = PromptConfig.get_config('intent_analysis')
            
            debug_print(f"âš™ï¸ ConfiguraciÃ³n OpenAI:\nğŸ¤– Modelo: {config['model']}\nğŸŒ¡ï¸ Temperature: {config['temperature']}\nğŸ“ Max tokens: {config['max_tokens']}", "analyze_intent", "openai_client.py")
            
            debug_print(f"ğŸ“ PROMPT ENVIADO A OPENAI:\n{prompt[:500]}{'...' if len(prompt) > 500 else ''}", "analyze_intent", "openai_client.py")
            
            debug_print("ğŸš€ Enviando peticiÃ³n a OpenAI...", "analyze_intent", "openai_client.py")
            response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                messages=[
                    {"role": "system", "content": "Eres un analizador de intenciÃ³n experto. Responde SOLO con JSON vÃ¡lido."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content
            if content:
                content = content.strip()
            else:
                content = ""
            debug_print(f"ğŸ“¥ RESPUESTA CRUDA DE OPENAI:\n{content}", "analyze_intent", "openai_client.py")
            
            # Intentar parsear JSON con limpieza de markdown
            try:
                debug_print("ğŸ”„ Parseando respuesta JSON...", "analyze_intent", "openai_client.py")
                
                # Limpiar markdown wrapping antes de parsear
                cleaned_content = clean_openai_json_response(content)
                debug_print(f"ğŸ§¹ CONTENIDO LIMPIO: {cleaned_content[:200]}{'...' if len(cleaned_content) > 200 else ''}", "analyze_intent", "openai_client.py")
                
                intent_data = json.loads(cleaned_content)
                debug_print(f"âœ… JSON PARSEADO EXITOSAMENTE!\nğŸ¯ CategorÃ­a: {intent_data.get('category', 'UNKNOWN')}\nğŸ“Š Confianza: {intent_data.get('confidence', 'N/A')}", "analyze_intent", "openai_client.py")
                return intent_data
            except json.JSONDecodeError as e:
                debug_print(f"âŒ ERROR PARSEANDO JSON: {e}\nğŸ“„ Contenido original: {content}\nğŸ“„ Contenido limpio: {cleaned_content if 'cleaned_content' in locals() else 'N/A'}", "analyze_intent", "openai_client.py")
                
                # Fallback con intenciÃ³n genÃ©rica
                debug_print("ğŸ”„ Usando respuesta FALLBACK genÃ©rica", "analyze_intent", "openai_client.py")
                return {
                    "category": "GENERAL_QUESTION",
                    "confidence": 0.5,
                    "should_ask_more": False,
                    "key_topics": ["general"],
                    "response_focus": "Responder de manera Ãºtil y amigable",
                    "recommended_action": "continue_conversation",
                    "urgency_level": "medium"
                }
                
        except Exception as e:
            debug_print(f"ğŸ’¥ ERROR CRÃTICO EN ANÃLISIS: {e}", "analyze_intent", "openai_client.py")
            import traceback
            debug_print(f"ğŸ“œ Traceback completo: {traceback.format_exc()}", "analyze_intent", "openai_client.py")
            
            # Fallback para asegurar que el bot funcione
            debug_print("ğŸš¨ Usando FALLBACK CRÃTICO", "analyze_intent", "openai_client.py")
            return {
                "category": "GENERAL_QUESTION",
                "confidence": 0.3,
                "should_ask_more": False,
                "key_topics": ["error"],
                "response_focus": "Responder de manera amigable",
                "recommended_action": "continue_conversation",
                "urgency_level": "low"
            }
    
    async def extract_information(
        self,
        user_message: str,
        user_memory
    ) -> Dict[str, Any]:
        """
        Extrae informaciÃ³n relevante del mensaje del usuario.
        
        Args:
            user_message: Mensaje del usuario
            user_memory: Contexto previo del usuario
            
        Returns:
            Dict con informaciÃ³n extraÃ­da estructurada
        """
        try:
            prompt = get_information_extraction_prompt(user_message, user_memory)
            config = PromptConfig.get_config('information_extraction')
            
            self.logger.info(f"ğŸ“Š Extrayendo informaciÃ³n de: '{user_message[:50]}...'")
            
            response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                messages=[
                    {"role": "system", "content": "Eres un extractor de informaciÃ³n experto. Responde SOLO con JSON vÃ¡lido."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content
            if content:
                content = content.strip()
            else:
                content = ""
            
            # Verificar si la respuesta estÃ¡ vacÃ­a
            if not content:
                self.logger.warning("âš ï¸ Respuesta vacÃ­a de OpenAI para extracciÃ³n de informaciÃ³n")
                return {}
            
            try:
                # Limpiar markdown wrapping antes de parsear
                cleaned_content = clean_openai_json_response(content)
                self.logger.info(f"ğŸ§¹ Contenido limpio: {cleaned_content[:100]}{'...' if len(cleaned_content) > 100 else ''}")
                
                extracted_data = json.loads(cleaned_content)
                self.logger.info(f"âœ… InformaciÃ³n extraÃ­da exitosamente")
                return extracted_data
            except json.JSONDecodeError as e:
                self.logger.error(f"âŒ Error parseando JSON de extracciÃ³n: {e}")
                self.logger.error(f"ğŸ“„ Contenido original: '{content}'")
                self.logger.error(f"ğŸ“„ Contenido limpio: '{cleaned_content if 'cleaned_content' in locals() else 'N/A'}'")
                return {}
                
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error en extracciÃ³n de informaciÃ³n: {e}")
            return {}
    
    async def generate_response(
        self,
        user_message: str,
        user_memory,
        intent_analysis: Dict[str, Any],
        context_info: str = ""
    ) -> str:
        """
        Genera una respuesta inteligente basada en intenciÃ³n y contexto.
        
        Args:
            user_message: Mensaje del usuario
            user_memory: Memoria del usuario
            intent_analysis: Resultado del anÃ¡lisis de intenciÃ³n
            context_info: InformaciÃ³n adicional de contexto
            
        Returns:
            Respuesta generada por GPT-4o-mini
        """
        try:
            prompt = get_response_generation_prompt(
                user_message, user_memory, intent_analysis, context_info
            )
            config = PromptConfig.get_config('main_agent')
            
            self.logger.info(f"ğŸ’¬ Generando respuesta para categorÃ­a: {intent_analysis.get('category', 'UNKNOWN')}")
            
            response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            generated_response = response.choices[0].message.content
            if generated_response:
                generated_response = generated_response.strip()
            else:
                generated_response = ""
            
            # Validar que la respuesta no estÃ© vacÃ­a
            if not generated_response:
                self.logger.warning("âš ï¸ Respuesta vacÃ­a de OpenAI, usando fallback")
                return self._get_fallback_response(intent_analysis.get('category'))
            
            self.logger.info(f"âœ… Respuesta generada: {len(generated_response)} caracteres")
            return generated_response
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error generando respuesta: {e}")
            # Fallback para asegurar que siempre responda algo
            return self._get_fallback_response(intent_analysis.get('category', 'GENERAL_QUESTION'))
    
    def _get_fallback_response(self, category: str) -> str:
        """
        Genera respuesta de fallback segÃºn la categorÃ­a.
        
        Args:
            category: CategorÃ­a de intenciÃ³n detectada
            
        Returns:
            Respuesta de fallback apropiada
        """
        fallback_responses = {
            'FREE_RESOURCES': """Â¡Por supuesto! ğŸ“š
            
Te voy a compartir algunos recursos gratuitos que te van a ayudar mucho.

Â¿Te gustarÃ­a que tambiÃ©n te cuente sobre nuestro curso completo?""",
            
            'EXPLORATION': """Â¡Excelente pregunta! ğŸ¯

Me encanta que estÃ©s explorando cÃ³mo la IA puede ayudarte.

Â¿En quÃ© Ã¡rea especÃ­fica te gustarÃ­a enfocarte? Â¿Marketing, automatizaciÃ³n, anÃ¡lisis de datos?""",
            
            'OBJECTION_PRICE': """Entiendo perfectamente tu preocupaciÃ³n por la inversiÃ³n. ğŸ’°

Lo que me gusta es que veas esto como una inversiÃ³n, no como un gasto.

Â¿Te gustarÃ­a que conversemos sobre el retorno de inversiÃ³n que puedes esperar?""",
            
            'CONTACT_REQUEST': """Â¡Perfecto! ğŸ‘¥

Te voy a conectar directamente con uno de nuestros asesores especializados.

Â¿EstÃ¡s listo/a para que iniciemos el contacto?""",
            
            'GENERAL_QUESTION': """Â¡Excelente pregunta! ğŸ˜Š

Me encanta tu curiosidad sobre la IA.

Â¿En quÃ© puedo ayudarte especÃ­ficamente? Â¿Te interesa automatizar algÃºn proceso en particular?"""
        }
        
        return fallback_responses.get(category, """Â¡Hola! ğŸ‘‹

Gracias por escribir. Estoy aquÃ­ para ayudarte con todo lo relacionado a nuestros cursos de IA.

Â¿En quÃ© puedo asistirte hoy?""")
    
    async def analyze_and_respond(
        self,
        user_message: str,
        user_memory,
        recent_messages: Optional[list] = None,
        context_info: str = ""
    ) -> Dict[str, Any]:
        """
        Proceso completo: analiza intenciÃ³n y genera respuesta.
        
        Args:
            user_message: Mensaje del usuario
            user_memory: Memoria del usuario
            recent_messages: Mensajes recientes
            context_info: InformaciÃ³n de contexto
            
        Returns:
            Dict con anÃ¡lisis e informaciÃ³n extraÃ­da y respuesta generada
        """
        try:
            # 1. Analizar intenciÃ³n
            intent_analysis = await self.analyze_intent(
                user_message, user_memory, recent_messages
            )
            
            # 2. Extraer informaciÃ³n (con manejo de errores mejorado)
            try:
                extracted_info = await self.extract_information(
                    user_message, user_memory
                )
            except Exception as extraction_error:
                self.logger.warning(f"âš ï¸ Error en extracciÃ³n de informaciÃ³n: {extraction_error}")
                extracted_info = {}
            
            # 3. Generar respuesta basada en intenciÃ³n
            response = await self.generate_response(
                user_message, user_memory, intent_analysis, context_info
            )
            
            return {
                'intent_analysis': intent_analysis,
                'extracted_info': extracted_info,
                'response': response,
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error en proceso completo: {e}")
            return {
                'intent_analysis': {'category': 'GENERAL_QUESTION', 'confidence': 0.3},
                'extracted_info': {},
                'response': self._get_fallback_response('GENERAL_QUESTION'),
                'success': False,
                'error': str(e)
            }
    
    async def validate_response(
        self,
        response: str,
        course_data: dict = None,
        bonuses_data: list = None,
        all_courses_data: list = None
    ) -> Dict[str, Any]:
        """
        Valida una respuesta usando el validador anti-alucinaciÃ³n.
        
        Args:
            response: Respuesta del agente a validar
            course_data: Datos del curso para validaciÃ³n
            bonuses_data: Lista de bonos disponibles
            all_courses_data: Lista de todos los cursos
            
        Returns:
            Dict con resultado de validaciÃ³n
        """
        try:
            debug_print(f"ğŸ” VALIDANDO RESPUESTA\nğŸ“ Texto: '{response[:100]}{'...' if len(response) > 100 else ''}'", "validate_response", "openai_client.py")
            
            prompt = get_validation_prompt(response, course_data or {}, bonuses_data or [], all_courses_data or [])
            config = PromptConfig.get_config('intent_analysis')  # Usar misma config que intent_analysis
            
            debug_print("ğŸš€ Enviando a OpenAI para validaciÃ³n...", "validate_response", "openai_client.py")
            validation_response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=0.1,  # Muy baja para validaciÃ³n precisa
                max_tokens=300,
                messages=[
                    {"role": "system", "content": "Eres un validador anti-alucinaciÃ³n. Responde SOLO con JSON vÃ¡lido."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = validation_response.choices[0].message.content.strip()
            debug_print(f"ğŸ“¥ RESPUESTA DE VALIDACIÃ“N:\n{content}", "validate_response", "openai_client.py")
            
            try:
                # Limpiar markdown wrapping antes de parsear
                cleaned_content = clean_openai_json_response(content)
                debug_print(f"ğŸ§¹ CONTENIDO VALIDACIÃ“N LIMPIO: {cleaned_content[:100]}{'...' if len(cleaned_content) > 100 else ''}", "validate_response", "openai_client.py")
                
                validation_data = json.loads(cleaned_content)
                debug_print(f"âœ… ValidaciÃ³n PARSEADA - Es vÃ¡lida: {validation_data.get('is_valid', True)}", "validate_response", "openai_client.py")
                return validation_data
            except json.JSONDecodeError as e:
                debug_print(f"âŒ ERROR PARSEANDO JSON DE VALIDACIÃ“N: {e}", "validate_response", "openai_client.py")
                debug_print(f"ğŸ“„ Contenido original: {content}", "validate_response", "openai_client.py")
                debug_print(f"ğŸ“„ Contenido limpio: {cleaned_content if 'cleaned_content' in locals() else 'N/A'}", "validate_response", "openai_client.py")
                # En caso de error de parseo, aprobar por defecto (filosofÃ­a permisiva)
                return {
                    "is_valid": True,
                    "confidence": 0.8,
                    "issues": [],
                    "corrected_response": None,
                    "explanation": "Error parseando validaciÃ³n, aprobado por defecto"
                }
                
        except Exception as e:
            debug_print(f"ğŸ’¥ ERROR EN VALIDACIÃ“N: {e}", "validate_response", "openai_client.py")
            # En caso de error, aprobar por defecto (filosofÃ­a permisiva)
            return {
                "is_valid": True,
                "confidence": 0.7,
                "issues": [],
                "corrected_response": None,
                "explanation": f"Error en validaciÃ³n, aprobado por defecto: {str(e)}"
            }