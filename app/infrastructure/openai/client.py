"""
Cliente OpenAI para integraci√≥n con GPT-4o-mini.
Maneja an√°lisis de intenci√≥n, extracci√≥n de informaci√≥n y generaci√≥n de respuestas.
"""
import logging
import json
from typing import Dict, Any, Optional
from openai import AsyncOpenAI

from app.config import settings
from prompts.agent_prompts import (
    get_intent_analysis_prompt,
    get_information_extraction_prompt,
    get_response_generation_prompt,
    get_validation_prompt,
    PromptConfig
)

logger = logging.getLogger(__name__)

def debug_print(message: str, function_name: str = "", file_name: str = "openai_client.py"):
    """Print de debug visual para consola"""
    print(f"ü§ñ [{file_name}::{function_name}] {message}")


class OpenAIClient:
    """
    Cliente especializado para interacciones con OpenAI GPT-4o-mini.
    
    Responsabilidades:
    - An√°lisis de intenci√≥n de mensajes
    - Extracci√≥n de informaci√≥n de usuarios
    - Generaci√≥n de respuestas inteligentes
    - Manejo de errores y configuraci√≥n
    """
    
    def __init__(self):
        """Inicializa el cliente OpenAI con configuraci√≥n."""
        if not settings.openai_api_key:
            raise ValueError("OPENAI_API_KEY no est√° configurada")
        
        self.client = AsyncOpenAI(
            api_key=settings.openai_api_key
        )
        self.logger = logging.getLogger(__name__)
    
    async def analyze_intent(
        self,
        user_message: str,
        user_memory,
        recent_messages: Optional[list] = None
    ) -> Dict[str, Any]:
        """
        Analiza la intenci√≥n del mensaje del usuario.
        
        Args:
            user_message: Mensaje del usuario a analizar
            user_memory: Memoria del usuario con contexto
            recent_messages: Mensajes recientes para contexto
            
        Returns:
            Dict con an√°lisis de intenci√≥n estructurado
        """
        try:
            debug_print(f"üîç ANALIZANDO INTENCI√ìN\nüí¨ Mensaje: '{user_message}'\nüë§ Usuario: {user_memory.name if user_memory.name else 'An√≥nimo'}", "analyze_intent", "openai_client.py")
            
            prompt = get_intent_analysis_prompt(user_message, user_memory, recent_messages)
            config = PromptConfig.get_config('intent_analysis')
            
            debug_print(f"‚öôÔ∏è Configuraci√≥n OpenAI:\nü§ñ Modelo: {config['model']}\nüå°Ô∏è Temperature: {config['temperature']}\nüìè Max tokens: {config['max_tokens']}", "analyze_intent", "openai_client.py")
            
            debug_print(f"üìù PROMPT ENVIADO A OPENAI:\n{prompt[:500]}{'...' if len(prompt) > 500 else ''}", "analyze_intent", "openai_client.py")
            
            debug_print("üöÄ Enviando petici√≥n a OpenAI...", "analyze_intent", "openai_client.py")
            response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                messages=[
                    {"role": "system", "content": "Eres un analizador de intenci√≥n experto. Responde SOLO con JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content
            if content:
                content = content.strip()
            else:
                content = ""
            debug_print(f"üì• RESPUESTA CRUDA DE OPENAI:\n{content}", "analyze_intent", "openai_client.py")
            
            # Intentar parsear JSON
            try:
                debug_print("üîÑ Parseando respuesta JSON...", "analyze_intent", "openai_client.py")
                intent_data = json.loads(content)
                debug_print(f"‚úÖ JSON PARSEADO EXITOSAMENTE!\nüéØ Categor√≠a: {intent_data.get('category', 'UNKNOWN')}\nüìä Confianza: {intent_data.get('confidence', 'N/A')}", "analyze_intent", "openai_client.py")
                return intent_data
            except json.JSONDecodeError as e:
                debug_print(f"‚ùå ERROR PARSEANDO JSON: {e}\nüìÑ Respuesta recibida: {content}", "analyze_intent", "openai_client.py")
                
                # Fallback con intenci√≥n gen√©rica
                debug_print("üîÑ Usando respuesta FALLBACK gen√©rica", "analyze_intent", "openai_client.py")
                return {
                    "category": "GENERAL_QUESTION",
                    "confidence": 0.5,
                    "should_ask_more": False,
                    "key_topics": ["general"],
                    "response_focus": "Responder de manera √∫til y amigable",
                    "recommended_action": "continue_conversation",
                    "urgency_level": "medium"
                }
                
        except Exception as e:
            debug_print(f"üí• ERROR CR√çTICO EN AN√ÅLISIS: {e}", "analyze_intent", "openai_client.py")
            import traceback
            debug_print(f"üìú Traceback completo: {traceback.format_exc()}", "analyze_intent", "openai_client.py")
            
            # Fallback para asegurar que el bot funcione
            debug_print("üö® Usando FALLBACK CR√çTICO", "analyze_intent", "openai_client.py")
            return {
                "category": "GENERAL_QUESTION",
                "confidence": 0.3,
                "should_ask_more": False,
                "key_topics": ["error"],
                "response_focus": "Responder de manera amigable",
                "recommended_action": "continue_conversation",
                "urgency_level": "low"
            }
    
    async def extract_information(
        self,
        user_message: str,
        user_memory
    ) -> Dict[str, Any]:
        """
        Extrae informaci√≥n relevante del mensaje del usuario.
        
        Args:
            user_message: Mensaje del usuario
            user_memory: Contexto previo del usuario
            
        Returns:
            Dict con informaci√≥n extra√≠da estructurada
        """
        try:
            prompt = get_information_extraction_prompt(user_message, user_memory)
            config = PromptConfig.get_config('information_extraction')
            
            self.logger.info(f"üìä Extrayendo informaci√≥n de: '{user_message[:50]}...'")
            
            response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                messages=[
                    {"role": "system", "content": "Eres un extractor de informaci√≥n experto. Responde SOLO con JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content
            if content:
                content = content.strip()
            else:
                content = ""
            
            # Verificar si la respuesta est√° vac√≠a
            if not content:
                self.logger.warning("‚ö†Ô∏è Respuesta vac√≠a de OpenAI para extracci√≥n de informaci√≥n")
                return {}
            
            try:
                extracted_data = json.loads(content)
                self.logger.info(f"‚úÖ Informaci√≥n extra√≠da exitosamente")
                return extracted_data
            except json.JSONDecodeError as e:
                self.logger.error(f"‚ùå Error parseando JSON de extracci√≥n: {e}")
                self.logger.error(f"üìÑ Contenido recibido: '{content}'")
                return {}
                
        except Exception as e:
            self.logger.error(f"üí• Error en extracci√≥n de informaci√≥n: {e}")
            return {}
    
    async def generate_response(
        self,
        user_message: str,
        user_memory,
        intent_analysis: Dict[str, Any],
        context_info: str = ""
    ) -> str:
        """
        Genera una respuesta inteligente basada en intenci√≥n y contexto.
        
        Args:
            user_message: Mensaje del usuario
            user_memory: Memoria del usuario
            intent_analysis: Resultado del an√°lisis de intenci√≥n
            context_info: Informaci√≥n adicional de contexto
            
        Returns:
            Respuesta generada por GPT-4o-mini
        """
        try:
            prompt = get_response_generation_prompt(
                user_message, user_memory, intent_analysis, context_info
            )
            config = PromptConfig.get_config('main_agent')
            
            self.logger.info(f"üí¨ Generando respuesta para categor√≠a: {intent_analysis.get('category', 'UNKNOWN')}")
            
            response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            generated_response = response.choices[0].message.content
            if generated_response:
                generated_response = generated_response.strip()
            else:
                generated_response = ""
            
            # Validar que la respuesta no est√© vac√≠a
            if not generated_response:
                self.logger.warning("‚ö†Ô∏è Respuesta vac√≠a de OpenAI, usando fallback")
                return self._get_fallback_response(intent_analysis.get('category'))
            
            self.logger.info(f"‚úÖ Respuesta generada: {len(generated_response)} caracteres")
            return generated_response
            
        except Exception as e:
            self.logger.error(f"üí• Error generando respuesta: {e}")
            # Fallback para asegurar que siempre responda algo
            return self._get_fallback_response(intent_analysis.get('category', 'GENERAL_QUESTION'))
    
    def _get_fallback_response(self, category: str) -> str:
        """
        Genera respuesta de fallback seg√∫n la categor√≠a.
        
        Args:
            category: Categor√≠a de intenci√≥n detectada
            
        Returns:
            Respuesta de fallback apropiada
        """
        fallback_responses = {
            'FREE_RESOURCES': """¬°Por supuesto! üìö
            
Te voy a compartir algunos recursos gratuitos que te van a ayudar mucho.

¬øTe gustar√≠a que tambi√©n te cuente sobre nuestro curso completo?""",
            
            'EXPLORATION': """¬°Excelente pregunta! üéØ

Me encanta que est√©s explorando c√≥mo la IA puede ayudarte.

¬øEn qu√© √°rea espec√≠fica te gustar√≠a enfocarte? ¬øMarketing, automatizaci√≥n, an√°lisis de datos?""",
            
            'OBJECTION_PRICE': """Entiendo perfectamente tu preocupaci√≥n por la inversi√≥n. üí∞

Lo que me gusta es que veas esto como una inversi√≥n, no como un gasto.

¬øTe gustar√≠a que conversemos sobre el retorno de inversi√≥n que puedes esperar?""",
            
            'CONTACT_REQUEST': """¬°Perfecto! üë•

Te voy a conectar directamente con uno de nuestros asesores especializados.

¬øEst√°s listo/a para que iniciemos el contacto?""",
            
            'GENERAL_QUESTION': """¬°Excelente pregunta! üòä

Me encanta tu curiosidad sobre la IA.

¬øEn qu√© puedo ayudarte espec√≠ficamente? ¬øTe interesa automatizar alg√∫n proceso en particular?"""
        }
        
        return fallback_responses.get(category, """¬°Hola! üëã

Gracias por escribir. Estoy aqu√≠ para ayudarte con todo lo relacionado a nuestros cursos de IA.

¬øEn qu√© puedo asistirte hoy?""")
    
    async def analyze_and_respond(
        self,
        user_message: str,
        user_memory,
        recent_messages: Optional[list] = None,
        context_info: str = ""
    ) -> Dict[str, Any]:
        """
        Proceso completo: analiza intenci√≥n y genera respuesta.
        
        Args:
            user_message: Mensaje del usuario
            user_memory: Memoria del usuario
            recent_messages: Mensajes recientes
            context_info: Informaci√≥n de contexto
            
        Returns:
            Dict con an√°lisis e informaci√≥n extra√≠da y respuesta generada
        """
        try:
            # 1. Analizar intenci√≥n
            intent_analysis = await self.analyze_intent(
                user_message, user_memory, recent_messages
            )
            
            # 2. Extraer informaci√≥n (con manejo de errores mejorado)
            try:
                extracted_info = await self.extract_information(
                    user_message, user_memory
                )
            except Exception as extraction_error:
                self.logger.warning(f"‚ö†Ô∏è Error en extracci√≥n de informaci√≥n: {extraction_error}")
                extracted_info = {}
            
            # 3. Generar respuesta basada en intenci√≥n
            response = await self.generate_response(
                user_message, user_memory, intent_analysis, context_info
            )
            
            return {
                'intent_analysis': intent_analysis,
                'extracted_info': extracted_info,
                'response': response,
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"üí• Error en proceso completo: {e}")
            return {
                'intent_analysis': {'category': 'GENERAL_QUESTION', 'confidence': 0.3},
                'extracted_info': {},
                'response': self._get_fallback_response('GENERAL_QUESTION'),
                'success': False,
                'error': str(e)
            }
    
    async def validate_response(
        self,
        response: str,
        course_data: dict = None,
        bonuses_data: list = None,
        all_courses_data: list = None
    ) -> Dict[str, Any]:
        """
        Valida una respuesta usando el validador anti-alucinaci√≥n.
        
        Args:
            response: Respuesta del agente a validar
            course_data: Datos del curso para validaci√≥n
            bonuses_data: Lista de bonos disponibles
            all_courses_data: Lista de todos los cursos
            
        Returns:
            Dict con resultado de validaci√≥n
        """
        try:
            debug_print(f"üîç VALIDANDO RESPUESTA\nüìù Texto: '{response[:100]}{'...' if len(response) > 100 else ''}'", "validate_response", "openai_client.py")
            
            prompt = get_validation_prompt(response, course_data or {}, bonuses_data or [], all_courses_data or [])
            config = PromptConfig.get_config('intent_analysis')  # Usar misma config que intent_analysis
            
            debug_print("üöÄ Enviando a OpenAI para validaci√≥n...", "validate_response", "openai_client.py")
            validation_response = await self.client.chat.completions.create(
                model=config['model'],
                temperature=0.1,  # Muy baja para validaci√≥n precisa
                max_tokens=300,
                messages=[
                    {"role": "system", "content": "Eres un validador anti-alucinaci√≥n. Responde SOLO con JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = validation_response.choices[0].message.content.strip()
            debug_print(f"üì• RESPUESTA DE VALIDACI√ìN:\n{content}", "validate_response", "openai_client.py")
            
            try:
                validation_data = json.loads(content)
                debug_print(f"‚úÖ Validaci√≥n PARSEADA - Es v√°lida: {validation_data.get('is_valid', True)}", "validate_response", "openai_client.py")
                return validation_data
            except json.JSONDecodeError as e:
                debug_print(f"‚ùå ERROR PARSEANDO JSON DE VALIDACI√ìN: {e}", "validate_response", "openai_client.py")
                # En caso de error de parseo, aprobar por defecto (filosof√≠a permisiva)
                return {
                    "is_valid": True,
                    "confidence": 0.8,
                    "issues": [],
                    "corrected_response": None,
                    "explanation": "Error parseando validaci√≥n, aprobado por defecto"
                }
                
        except Exception as e:
            debug_print(f"üí• ERROR EN VALIDACI√ìN: {e}", "validate_response", "openai_client.py")
            # En caso de error, aprobar por defecto (filosof√≠a permisiva)
            return {
                "is_valid": True,
                "confidence": 0.7,
                "issues": [],
                "corrected_response": None,
                "explanation": f"Error en validaci√≥n, aprobado por defecto: {str(e)}"
            }